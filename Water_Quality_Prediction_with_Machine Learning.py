# -*- coding: utf-8 -*-
"""Water Quality Prediction with Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l6mLZLmR2yNqp-Qq4rggarR2-m0qLFk1

## Task

Using the above dataset, Municipal Corporation in a particular city wants to construct a classification model that can label any new water sample as either safe or unsafe for drinking. 
Construct such a model in Python by trying two classification algorithms - random forest and support vector classification.
"""

# Importing Libraries

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

import warnings 
warnings.simplefilter('ignore')

# Reading Data

data = pd.read_csv('/content/drive/MyDrive/Practical Assignment â€“ Supervised Machine Learning/QualityPrediction.csv')

# Check first 5 rows
data.head()

# Checking rows and columns of dataset
data.shape

# Getting some basic information about dataset
data.info()

"""### Data Visualization"""

# Let's first check the class distribution

data['is_safe'].value_counts()

# Let's create a Pie-Chart to graphically represent the class distribution

import plotly.graph_objects as go

labels = ['Unsafe', 'Safe']

fig = go.Figure(data=[go.Pie(labels=labels, values=data['is_safe'].value_counts().values, pull=[0, 0.05])])
fig.show()

"""***The majority of water samples in a dataset are unsafe, around 88.6% water samples in a dataset are unsafe whereas 11.4% water samples are safe.***"""

data.columns

"""## 1. Data Preparation

#### Handling Categorical Data
"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

data['is_safe'] = le.fit_transform(data['is_safe'])

data.head()

## Data Splitting

X = data.drop('is_safe', axis = 1)
y = data['is_safe']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 20)

X_train.shape, X_test.shape

"""## Model Building

#### 1. Support Vector Machine
"""

# Support Vector Machine
from sklearn import svm
classifier = svm.SVC(kernel = 'rbf')

#fit the model
classifier.fit(X_train, y_train)

# Make a prediction for the testing set
y_pred_te = classifier.predict(X_test)

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)

# Making prediction on test data

pred_rf = rf.predict(X_test)

"""## Model Evaluation

Since, it's a binary classification task and the dataset is imbalanced so we are going to use Precision, Recall, F1-score and ROC-AUC score to evaluate the performance of the model.
"""

from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score

# print the Precision Score on test data

print("Precision Score with SVM:", precision_score(y_test, y_pred_te))
print("Precision Score with Random Forest:", precision_score(y_test, pred_rf))

# print the Recall Score on test data

print("Recall Score with SVM:", recall_score(y_test, y_pred_te))
print("Recall Score with Random Forest:", recall_score(y_test, pred_rf))

# print the F1-score on test data

print("F1-Score with SVM:", f1_score(y_test, y_pred_te))
print("F1-Score with Random Forest:", f1_score(y_test, pred_rf))

# print the AUC-score on test data

print("AUC ROC Score with SVM:", roc_auc_score(y_test, y_pred_te))
print("AUC ROC Score with Random Forest:", roc_auc_score(y_test, pred_rf))

"""***From the above evaluation, we can observe that ROC score of SVM is 50%, which signifies that the SVM is not well fitted on model and is not able to distinguish between positive and negative class whereas the Random Forest algorithm is able to achieve an ROC-AUC score of 83.2%, which indicates the model is properly able to distunguish between positive and negative class. So, let's try to tune the hyperparameters of random forest algorithm to boost the performance of the model.***

## 2. Model Hyper Parameter Tunning

We are going to use Randomized Search CV to tune the hyperparameters of Random Forest classifier as it computationally faster as compared to another tunning methods. So, we should use such tunning method which is having good computational speed especially when we are dealing with the large data set.
"""

from sklearn.model_selection import RandomizedSearchCV

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]

# Number of features to consider at every split
max_features = ['auto', 'sqrt', 'log2']

# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(3, 50, 20)]

# Minimum number of samples required to split a node
min_samples_split = [2, 5, 8,10, 12]

# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4,6,8]


random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
              'criterion':['entropy', 'gini impurity']}

model_rscv = RandomizedSearchCV(estimator = rf, param_distributions=random_grid, n_iter = 100, cv = 5, 
                                n_jobs = -1, random_state = 42)

model_rscv.fit(X_train, y_train)

model_rscv.best_params_

best_random = model_rscv.best_estimator_
best_random

# Making Prediction

y_pred_rf_rscv = best_random.predict(X_test)

"""## 3. Model Evaluation with AUC Score

We are going to use ROC AUC Score for the evaluation of the model as we should use it when we care equally about positive and negative classes especially in case of imbalanced data. So, since the dataset is imbalanced and we need to take care about true negatives as much as we care about true positives then it totally makes sense to use ROC AUC.
"""

# Evaluation

from sklearn.metrics import confusion_matrix, classification_report
print("Precision Score: ", precision_score(y_test, y_pred_rf_rscv))
print("Recall Score: ", recall_score(y_test, y_pred_rf_rscv))
print("F1-Score: ", f1_score(y_test, y_pred_rf_rscv))
print("AUC Score: ", roc_auc_score(y_test, y_pred_rf_rscv))
print(classification_report(y_test, y_pred_rf_rscv))

"""## 4. Feature Selection to avoid Overfitting

Since the dataset contains so many columns so there might be a possibility that only few columns are impacting the target variable but we are using all the feature to build the model and probably leads to overfitting. So, to avoid overfitting we can simply drop those features which are least important to build the model. 
"""

# view the feature scores

feature_scores = pd.Series(rf.feature_importances_, index=X_train.columns, ).sort_values(ascending=False)
feature_scores

# Plot the bar graph to check of the features visually
feature_scores.plot(kind="bar",figsize=(16, 9))

X_train

X_train.columns

# drop the least important feature from X_train and X_test

X_train = X_train.drop(labels = ['flouride', 'selenium', 'mercury'], axis =  1)
X_test = X_test.drop(labels = ['flouride', 'selenium', 'mercury'], axis = 1)

X_test

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(n_estimators = 100, criterion='entropy', max_depth=50, 
                                             min_samples_split = 2, min_samples_leaf = 1, max_features= 'sqrt')
model.fit(X_train, y_train)

# Making Prediction

y_pred_rf_sel = model.predict(X_test)

# Evaluation

from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, roc_auc_score, auc
print("Precision Score: ", precision_score(y_test, y_pred_rf_sel))
print("Recall Score: ", recall_score(y_test, y_pred_rf_sel))
print("F1-Score: ", f1_score(y_test, y_pred_rf_sel))
print("AUC Score: ", roc_auc_score(y_test, y_pred_rf_sel))
print(classification_report(y_test, y_pred_rf_sel))

"""### Plot the ROC curve"""

from sklearn import metrics

plt.figure(figsize = (13, 7))
false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_pred_rf_sel)
roc_auc = auc(false_positive_rate, true_positive_rate)
plt.plot(false_positive_rate, true_positive_rate,label='AUC Level = %0.2f' % (roc_auc))
plt.legend(loc='lower right')
plt.plot([0, 1], [0, 1], 'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""## 5. Result Analysis

***a). Out of these two classification models, we can recommend Random Forest for deployment in the real-world.***

***b). Yes, SVM is underfitting and possible reason for this behaviour of the model is that the data is imbalanced and the model might not able to learn the pattern present in the minority class.***
"""

